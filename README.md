<div align="center">
<h1> MathEquiv dataset</h1> 
</div>

<p align="center">
<a href="https://opensource.org/licenses/Apache-2.0">
  <img src="https://img.shields.io/badge/License-Apache_2.0-green.svg"></a> 
<a href="https://github.com/Lolo1222/EquivPruner/pulls">
    <img src="https://img.shields.io/badge/Contributions-welcome-blue.svg?style=flat"></a>
</p>

## Dataset description
MathEquiv dataset is accompanied to [EquivPruner](https://anonymous.4open.science/r/EquivPruner-2364/) . It is specifically designed for **mathematical statement equivalence** , serving as a versatile resource applicable to a variety of mathematical tasks and scenarios. It consists of almost 100k math sentences pair with equivalence result and reasoning step generated by GPT-4O.

The dataset consists of three splits:

- `train` with 77.6k problems for training.
- `test` with 9.83k samples for testing.
- `valid` with 9.75k samples for validation.

You can load the dataset as follows:

```python
from datasets import load_dataset

ds = load_dataset("Jiawei1222/MathEquiv")
```

## Dataset curation

We constructed the MathEquiv dataset for mathematical statement equivalence. The foundation of this dataset consists of 7,500 mathematical problems sourced from the MATH training set. To prevent data leakage between training, validation, and test phases of EquivPruner, these 7,500 problems were first split into training, validation, and test sets using an 8:1:1 ratio, respectively. For each problem in these distinct sets, we generated candidate reasoning step pairs using the Qwen2.5-Math-7B-Instruct model  via Step-level Beam Search. These pairs were subsequently filtered based on Levenshtein distance, and a balanced sample from each set was then annotated for equivalence by GPT-4o. The promt is :

```markdown
Please determine whether the following two sentences are semanticly equivalent, and return 0: Not equivalent at all; 1: May not be equivalent; 2: Can't judge; 3: May be equivalent; 4: Exactly equivalent.
Please explain the reason, reflect, and provide a more accurate result.
Please output in the following Python dictionary format:
{
    "reasoning_step": "The reasoning process of the model",
    "result": "Final result" (int)
}
Question:
\textbf{Sentence1: }
The inner sum is a geometric series with the first term \frac{{1}}{{2^{{k^2-k+1}}}} and common ratio \frac{{1}}{{2}}, and it has 2k terms. The sum of a geometric series is given by:\sum_{{n=a}}^{{a+b-1}} ar^n = a \frac{{1-r^b}}{{1-r}}

\textbf{Sentence2: }
The inner sum \sum_{{n=k^2-k+1}}^{{k^2+k}} \frac{{1}}{{2^n}} is a geometric series with the first term \frac{{1}}{{2^{{k^2-k+1}}}} and common ratio \frac{{1}}{{2}}, and it has 2k terms. The sum of a geometric series is given by:\sum_{{n=a}}^{{a+r-1}} ar^n = a \frac{{1-r^r}}{{1-r}}

Output:
{
    "reasoning\_step": "Sentence1 uses the formula \sum_{{n=a}}^{{a+b-1}} ar^n = a \frac{{1-r^b}}{{1-r}}, while Sentence2 uses \sum_{{n=a}}^{{a+r-1}} ar^n = a \frac{{1-r^r}}{{1-r}}. The variables and the structure of the formulas are different, which affects the semantic equivalence. Additionally, the limits of the summation in Sentence2 are explicitly given as \sum_{{n=k^2-k+1}}^{{k^2+k}} \frac{{1}}{{2^n}}, which is not present in Sentence1. These differences indicate that the sentences are not semantically equivalent.",
    "result": 0
}
Question:
\textbf{Sentence1: }
The amplitude is the distance from the midline to the maximum value (or the minimum value). Therefore, we can calculate the amplitude as follows:{{Amplitude}} = {{Maximum value}} - {{Midline}} = 3 - 1 = 2 Since the amplitude is |a|, we have:|a| = 2.  Since a is a positive constant, we conclude:a = 2

\textbf{Sentence2: }
The amplitude of the sine wave is half the distance between the maximum value and the minimum value. Therefore, we can calculate the amplitude as follows:{{Amplitude}} = \frac{{{{Maximum value}} - {{Minimum value}}}}{{2}} = \frac{{3 - (-1)}}{{2}} = \frac{{3 + 1}}{{2}} = \frac{{4}}{{2}} = 2 Since the amplitude is |a|, we have:|a| = 2. Since a is a positive constant, we conclude: a = 2

Output:
{
    "reasoning_step": "Sentence1 defines the amplitude as the distance from the midline to the maximum value, calculating it as ({{Amplitude}} = {{Maximum value}} - {{Midline}} = 3 - 1 = 2). Sentence2 defines the amplitude as half the distance between the maximum and minimum values, calculating it as {{Amplitude}} = \frac{{{{Maximum value}} - {{Minimum value}}}}{{2}} = \frac{{3 - (-1)}}{{2}} = 2). While both methods yield the same result (a = 2), the definitions and calculations are fundamentally different. This difference in methodology means the sentences are not semantically equivalent.",
    "result": 0
}
Question:
textbf{Sentence1:} 
{sentence1}
textbf{Sentence2:}
{sentence2}

Output:
```

## License

The dataset is licensed under Apache 2.0